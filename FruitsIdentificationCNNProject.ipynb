{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e8a1dd6-1f79-4db4-83c7-02adcc44be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6283f2e5-f43a-4ee2-9d92-ecf7f6e3ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define paths\n",
    "train_dir = 'C:/Users/Lindiway/MACHINE_LEARNING_2025/FEB-ML-25-Multi-Partner/Cnn_Rnn/fruits-360/Training'\n",
    "test_dir = 'C:/Users/Lindiway/MACHINE_LEARNING_2025/FEB-ML-25-Multi-Partner/Cnn_Rnn/fruits-360/Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cafe1d4f-8adf-40c9-85df-48dd3d10af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Images - Rescaling and Expanding the Images \n",
    "#Initialize ImageDataGenerators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.3)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "219081ee-8dae-4c12-9c6a-a8f90c41816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71863 images belonging to 200 classes.\n",
      "Found 30693 images belonging to 200 classes.\n",
      "Found 34155 images belonging to 199 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(100, 100), batch_size=32, class_mode='binary', subset='training')\n",
    "validation_generator = train_datagen.flow_from_directory(train_dir, target_size=(100, 100), batch_size=32, class_mode='binary', subset='validation')\n",
    "test_generator = train_datagen.flow_from_directory(test_dir, target_size=(100, 100), batch_size=32, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee098d6-b178-4c61-b381-76906e5e23a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d2503e9-e99c-44f0-b347-7a2088901a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lindiway\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(filters=16, kernel_size=(2, 2), activation='relu', input_shape=(100, 100, 3)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "\n",
    "    \n",
    "    layers.Conv2D(filters=32, kernel_size=(2, 2), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "\n",
    "    layers.Conv2D(filters=64, kernel_size=(2, 2), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(150, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "\n",
    "    layers.Dense(1, activation='softmax'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90a798e5-b231-4f26-bde3-2e960db04a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "992130c8-3809-4192-bcd7-2b276c1aacef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lindiway\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lindiway\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\nn.py:908: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lindiway\\anaconda3\\Lib\\site-packages\\keras\\src\\losses\\losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1667s\u001b[0m 742ms/step - accuracy: 0.0035 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 2/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1294s\u001b[0m 576ms/step - accuracy: 0.0041 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 3/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 168ms/step - accuracy: 0.0041 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 4/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 90ms/step - accuracy: 0.0041 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 5/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 85ms/step - accuracy: 0.0042 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 6/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 88ms/step - accuracy: 0.0040 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 7/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 89ms/step - accuracy: 0.0042 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 8/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 87ms/step - accuracy: 0.0040 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 9/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 88ms/step - accuracy: 0.0042 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 10/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 88ms/step - accuracy: 0.0038 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 11/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 88ms/step - accuracy: 0.0042 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 12/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 88ms/step - accuracy: 0.0040 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 13/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 89ms/step - accuracy: 0.0043 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 14/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 87ms/step - accuracy: 0.0044 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 15/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 89ms/step - accuracy: 0.0038 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 16/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 89ms/step - accuracy: 0.0044 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 17/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 92ms/step - accuracy: 0.0039 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 18/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 91ms/step - accuracy: 0.0041 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 19/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 87ms/step - accuracy: 0.0044 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 20/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 89ms/step - accuracy: 0.0047 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 21/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 88ms/step - accuracy: 0.0042 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 22/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 91ms/step - accuracy: 0.0040 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 23/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 94ms/step - accuracy: 0.0043 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 24/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 91ms/step - accuracy: 0.0040 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 25/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 120ms/step - accuracy: 0.0048 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 26/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 92ms/step - accuracy: 0.0050 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 27/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 96ms/step - accuracy: 0.0045 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 28/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 137ms/step - accuracy: 0.0043 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 29/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 97ms/step - accuracy: 0.0043 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n",
      "Epoch 30/30\n",
      "\u001b[1m2246/2246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 137ms/step - accuracy: 0.0041 - loss: 0.0000e+00 - val_accuracy: 0.0042 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "history = model.fit(train_generator, epochs=30, validation_data = validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b70ca89-9df8-4b01-808d-43e3ea8887cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1068/1068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m473s\u001b[0m 443ms/step - accuracy: 0.0042 - loss: 0.0000e+00\n",
      "\n",
      "Test Accuracy: 0.42%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6ab879-b7df-4f39-98a7-b20a6fc18980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   6/1068\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 27ms/step  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lindiway\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\nn.py:908: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1068/1068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 43ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict and create confusion matrix\n",
    "y_true = test_generator.classes\n",
    "y_pred_prob = model.predict(test_generator)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbab361c-9c49-4900-a0ea-9451a598ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[  0 231   0 ...   0   0   0]\n",
      " [  0 142   0 ...   0   0   0]\n",
      " [  0 154   0 ...   0   0   0]\n",
      " ...\n",
      " [  0 157   0 ...   0   0   0]\n",
      " [  0  80   0 ...   0   0   0]\n",
      " [  0  80   0 ...   0   0   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       231\n",
      "           1       0.00      1.00      0.01       142\n",
      "           2       0.00      0.00      0.00       154\n",
      "           3       0.00      0.00      0.00       235\n",
      "           4       0.00      0.00      0.00       154\n",
      "           5       0.00      0.00      0.00       201\n",
      "           6       0.00      0.00      0.00       160\n",
      "           7       0.00      0.00      0.00       241\n",
      "           8       0.00      0.00      0.00       146\n",
      "           9       0.00      0.00      0.00       157\n",
      "          10       0.00      0.00      0.00       229\n",
      "          11       0.00      0.00      0.00       228\n",
      "          12       0.00      0.00      0.00       231\n",
      "          13       0.00      0.00      0.00       164\n",
      "          14       0.00      0.00      0.00        78\n",
      "          15       0.00      0.00      0.00       148\n",
      "          16       0.00      0.00      0.00       160\n",
      "          17       0.00      0.00      0.00       164\n",
      "          18       0.00      0.00      0.00       161\n",
      "          19       0.00      0.00      0.00       164\n",
      "          20       0.00      0.00      0.00       152\n",
      "          21       0.00      0.00      0.00       164\n",
      "          22       0.00      0.00      0.00       164\n",
      "          23       0.00      0.00      0.00       144\n",
      "          24       0.00      0.00      0.00       166\n",
      "          25       0.00      0.00      0.00       164\n",
      "          26       0.00      0.00      0.00       219\n",
      "          27       0.00      0.00      0.00       162\n",
      "          28       0.00      0.00      0.00       234\n",
      "          29       0.00      0.00      0.00       231\n",
      "          30       0.00      0.00      0.00       164\n",
      "          31       0.00      0.00      0.00       143\n",
      "          32       0.00      0.00      0.00       231\n",
      "          33       0.00      0.00      0.00       231\n",
      "          34       0.00      0.00      0.00       166\n",
      "          35       0.00      0.00      0.00       166\n",
      "          36       0.00      0.00      0.00        77\n",
      "          37       0.00      0.00      0.00       152\n",
      "          38       0.00      0.00      0.00       166\n",
      "          39       0.00      0.00      0.00        77\n",
      "          40       0.00      0.00      0.00       150\n",
      "          41       0.00      0.00      0.00       150\n",
      "          42       0.00      0.00      0.00        75\n",
      "          43       0.00      0.00      0.00       150\n",
      "          44       0.00      0.00      0.00       225\n",
      "          45       0.00      0.00      0.00       154\n",
      "          46       0.00      0.00      0.00        49\n",
      "          47       0.00      0.00      0.00        47\n",
      "          48       0.00      0.00      0.00       166\n",
      "          49       0.00      0.00      0.00       231\n",
      "          50       0.00      0.00      0.00       237\n",
      "          51       0.00      0.00      0.00       164\n",
      "          52       0.00      0.00      0.00       164\n",
      "          53       0.00      0.00      0.00       166\n",
      "          54       0.00      0.00      0.00        50\n",
      "          55       0.00      0.00      0.00       234\n",
      "          56       0.00      0.00      0.00       320\n",
      "          57       0.00      0.00      0.00       164\n",
      "          58       0.00      0.00      0.00       246\n",
      "          59       0.00      0.00      0.00       231\n",
      "          60       0.00      0.00      0.00       231\n",
      "          61       0.00      0.00      0.00       228\n",
      "          62       0.00      0.00      0.00       246\n",
      "          63       0.00      0.00      0.00       231\n",
      "          64       0.00      0.00      0.00       228\n",
      "          65       0.00      0.00      0.00       150\n",
      "          66       0.00      0.00      0.00       164\n",
      "          67       0.00      0.00      0.00       164\n",
      "          68       0.00      0.00      0.00       150\n",
      "          69       0.00      0.00      0.00       150\n",
      "          70       0.00      0.00      0.00       164\n",
      "          71       0.00      0.00      0.00       225\n",
      "          72       0.00      0.00      0.00        76\n",
      "          73       0.00      0.00      0.00       153\n",
      "          74       0.00      0.00      0.00       166\n",
      "          75       0.00      0.00      0.00       166\n",
      "          76       0.00      0.00      0.00       150\n",
      "          77       0.00      0.00      0.00       154\n",
      "          78       0.00      0.00      0.00        50\n",
      "          79       0.00      0.00      0.00       240\n",
      "          80       0.00      0.00      0.00        78\n",
      "          81       0.00      0.00      0.00        81\n",
      "          82       0.00      0.00      0.00        76\n",
      "          83       0.00      0.00      0.00       234\n",
      "          84       0.00      0.00      0.00       156\n",
      "          85       0.00      0.00      0.00       154\n",
      "          86       0.00      0.00      0.00       130\n",
      "          87       0.00      0.00      0.00       156\n",
      "          88       0.00      0.00      0.00       166\n",
      "          89       0.00      0.00      0.00       156\n",
      "          90       0.00      0.00      0.00        80\n",
      "          91       0.00      0.00      0.00       234\n",
      "          92       0.00      0.00      0.00        99\n",
      "          93       0.00      0.00      0.00       154\n",
      "          94       0.00      0.00      0.00       166\n",
      "          95       0.00      0.00      0.00       328\n",
      "          96       0.00      0.00      0.00       164\n",
      "          97       0.00      0.00      0.00       166\n",
      "          98       0.00      0.00      0.00       166\n",
      "          99       0.00      0.00      0.00       164\n",
      "         100       0.00      0.00      0.00       158\n",
      "         101       0.00      0.00      0.00       166\n",
      "         102       0.00      0.00      0.00       164\n",
      "         103       0.00      0.00      0.00       166\n",
      "         104       0.00      0.00      0.00       157\n",
      "         105       0.00      0.00      0.00       166\n",
      "         106       0.00      0.00      0.00       166\n",
      "         107       0.00      0.00      0.00       156\n",
      "         108       0.00      0.00      0.00       157\n",
      "         109       0.00      0.00      0.00       166\n",
      "         110       0.00      0.00      0.00       164\n",
      "         111       0.00      0.00      0.00       166\n",
      "         112       0.00      0.00      0.00       166\n",
      "         113       0.00      0.00      0.00       166\n",
      "         114       0.00      0.00      0.00       166\n",
      "         115       0.00      0.00      0.00       166\n",
      "         116       0.00      0.00      0.00       142\n",
      "         117       0.00      0.00      0.00       102\n",
      "         118       0.00      0.00      0.00       166\n",
      "         119       0.00      0.00      0.00       246\n",
      "         120       0.00      0.00      0.00       164\n",
      "         121       0.00      0.00      0.00       164\n",
      "         122       0.00      0.00      0.00       160\n",
      "         123       0.00      0.00      0.00       218\n",
      "         124       0.00      0.00      0.00       178\n",
      "         125       0.00      0.00      0.00       150\n",
      "         126       0.00      0.00      0.00       155\n",
      "         127       0.00      0.00      0.00       146\n",
      "         128       0.00      0.00      0.00       160\n",
      "         129       0.00      0.00      0.00       164\n",
      "         130       0.00      0.00      0.00       166\n",
      "         131       0.00      0.00      0.00       164\n",
      "         132       0.00      0.00      0.00       246\n",
      "         133       0.00      0.00      0.00       164\n",
      "         134       0.00      0.00      0.00       164\n",
      "         135       0.00      0.00      0.00       232\n",
      "         136       0.00      0.00      0.00        72\n",
      "         137       0.00      0.00      0.00       166\n",
      "         138       0.00      0.00      0.00       234\n",
      "         139       0.00      0.00      0.00       102\n",
      "         140       0.00      0.00      0.00       166\n",
      "         141       0.00      0.00      0.00       222\n",
      "         142       0.00      0.00      0.00       237\n",
      "         143       0.00      0.00      0.00       166\n",
      "         144       0.00      0.00      0.00       166\n",
      "         145       0.00      0.00      0.00       148\n",
      "         146       0.00      0.00      0.00       234\n",
      "         147       0.00      0.00      0.00       222\n",
      "         148       0.00      0.00      0.00       222\n",
      "         149       0.00      0.00      0.00       164\n",
      "         150       0.00      0.00      0.00       164\n",
      "         151       0.00      0.00      0.00       166\n",
      "         152       0.00      0.00      0.00       163\n",
      "         153       0.00      0.00      0.00       231\n",
      "         154       0.00      0.00      0.00       166\n",
      "         155       0.00      0.00      0.00       151\n",
      "         156       0.00      0.00      0.00       142\n",
      "         157       0.00      0.00      0.00       304\n",
      "         158       0.00      0.00      0.00       164\n",
      "         159       0.00      0.00      0.00       153\n",
      "         160       0.00      0.00      0.00       150\n",
      "         161       0.00      0.00      0.00       151\n",
      "         162       0.00      0.00      0.00       150\n",
      "         163       0.00      0.00      0.00       150\n",
      "         164       0.00      0.00      0.00       166\n",
      "         165       0.00      0.00      0.00       240\n",
      "         166       0.00      0.00      0.00       242\n",
      "         167       0.00      0.00      0.00       249\n",
      "         168       0.00      0.00      0.00       164\n",
      "         169       0.00      0.00      0.00       166\n",
      "         170       0.00      0.00      0.00       164\n",
      "         171       0.00      0.00      0.00       162\n",
      "         172       0.00      0.00      0.00       164\n",
      "         173       0.00      0.00      0.00       246\n",
      "         174       0.00      0.00      0.00       166\n",
      "         175       0.00      0.00      0.00       166\n",
      "         176       0.00      0.00      0.00       246\n",
      "         177       0.00      0.00      0.00       228\n",
      "         178       0.00      0.00      0.00       225\n",
      "         179       0.00      0.00      0.00       246\n",
      "         180       0.00      0.00      0.00       160\n",
      "         181       0.00      0.00      0.00       222\n",
      "         182       0.00      0.00      0.00       231\n",
      "         183       0.00      0.00      0.00       239\n",
      "         184       0.00      0.00      0.00       239\n",
      "         185       0.00      0.00      0.00       100\n",
      "         186       0.00      0.00      0.00       100\n",
      "         187       0.00      0.00      0.00       164\n",
      "         188       0.00      0.00      0.00       100\n",
      "         189       0.00      0.00      0.00       100\n",
      "         190       0.00      0.00      0.00       228\n",
      "         191       0.00      0.00      0.00       127\n",
      "         192       0.00      0.00      0.00       147\n",
      "         193       0.00      0.00      0.00       153\n",
      "         194       0.00      0.00      0.00       158\n",
      "         195       0.00      0.00      0.00       249\n",
      "         196       0.00      0.00      0.00       157\n",
      "         197       0.00      0.00      0.00        80\n",
      "         198       0.00      0.00      0.00        80\n",
      "\n",
      "    accuracy                           0.00     34155\n",
      "   macro avg       0.00      0.01      0.00     34155\n",
      "weighted avg       0.00      0.00      0.00     34155\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lindiway\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Lindiway\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Lindiway\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix and Classification Report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9699caaa-76d6-4480-b174-75da0b92ed78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
